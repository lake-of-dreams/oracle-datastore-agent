"""
Schema Generator for Oracle Database 23ai
Generates optimal database schemas based on data analysis
"""
import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from analyzer import LLMDataAnalysis, DataType
from common.setup import setup_logging


setup_logging()
logger = logging.getLogger(__name__)


@dataclass
class TableSchema:
    """Database table schema definition"""
    table_name: str
    columns: List[Dict[str, Any]]
    indexes: List[Dict[str, Any]]
    constraints: List[Dict[str, Any]]
    oracle_23ai_features: List[str]


class SchemaGenerator:
    """Generates optimal database schemas for Oracle Database 23ai"""
    
    def __init__(self):
        self.table_counter = 0
    
    def generate_schema(self, data_analysis: LLMDataAnalysis, table_name: Optional[str] = None) -> TableSchema:
        """
        Generate database schema based on data analysis
        
        Args:
            data_analysis: Analysis result from DataAnalyzer
            table_name: Optional custom table name
            
        Returns:
            TableSchema object with complete schema definition
        """
        if not table_name:
            table_name = self._generate_table_name(data_analysis.data_type)
        
        if data_analysis.data_type == DataType.JSON:
            return self._generate_json_schema(data_analysis, table_name)
        elif data_analysis.data_type in [DataType.CSV, DataType.TSV]:
            return self._generate_tabular_schema(data_analysis, table_name)
        elif data_analysis.data_type == DataType.TEXT:
            return self._generate_text_schema(data_analysis, table_name)
        elif data_analysis.data_type == DataType.BINARY:
            return self._generate_binary_schema(data_analysis, table_name)
        else:
            return self._generate_generic_schema(data_analysis, table_name)
    
    def _generate_table_name(self, data_type: DataType) -> str:
        """Generate a unique table name based on data type"""
        self.table_counter += 1
        return f"{data_type.value}_data_{self.table_counter}"
    
    def _generate_json_schema(self, analysis: LLMDataAnalysis, table_name: str) -> TableSchema:
        """Generate schema for JSON data using Oracle 23ai features"""
        columns = [
            {
                "name": "id",
                "type": "NUMBER",
                "constraints": ["PRIMARY KEY"],
                "generated": "GENERATED BY DEFAULT AS IDENTITY"
            },
            {
                "name": "json_data",
                "type": "JSON",
                "constraints": ["NOT NULL"],
                "description": "JSON document storage"
            },
            {
                "name": "created_at",
                "type": "TIMESTAMP",
                "constraints": ["NOT NULL"],
                "default": "CURRENT_TIMESTAMP"
            },
            {
                "name": "updated_at",
                "type": "TIMESTAMP",
                "default": "CURRENT_TIMESTAMP"
            }
        ]
        
        # Add metadata columns if available
        if analysis.metadata:
            columns.append({
                "name": "metadata",
                "type": "JSON",
                "description": "Additional metadata"
            })
        
        indexes = [
            {
                "name": f"idx_{table_name}_json_path",
                "type": "FUNCTION_BASED",
                "columns": ["JSON_VALUE(json_data, '$.id')"],
                "description": "Index on JSON ID field"
            },
            {
                "name": f"idx_{table_name}_created",
                "type": "BTREE",
                "columns": ["created_at"]
            }
        ]
        
        # Add indexes for common JSON fields if schema is available
        if analysis.schema and "properties" in analysis.schema:
            for field_name in analysis.schema["properties"].keys():
                if field_name in ["id", "name", "status", "type"]:  # Common indexed fields
                    indexes.append({
                        "name": f"idx_{table_name}_{field_name}",
                        "type": "FUNCTION_BASED",
                        "columns": [f"JSON_VALUE(json_data, '$.{field_name}')"],
                        "description": f"Index on JSON {field_name} field"
                    })
        
        constraints = [
            {
                "name": f"chk_{table_name}_json_valid",
                "type": "CHECK",
                "condition": "json_data IS JSON",
                "description": "Ensure JSON is valid"
            }
        ]
        
        oracle_23ai_features = [
            "JSON Relational Duality",
            "JSON column type",
            "JSON_VALUE function for indexing",
            "IS JSON constraint"
        ]
        
        return TableSchema(
            table_name=table_name,
            columns=columns,
            indexes=indexes,
            constraints=constraints,
            oracle_23ai_features=oracle_23ai_features
        )
    
    def _generate_tabular_schema(self, analysis: LLMDataAnalysis, table_name: str) -> TableSchema:
        """Generate schema for CSV/TSV data"""
        columns = [
            {
                "name": "id",
                "type": "NUMBER",
                "constraints": ["PRIMARY KEY"],
                "generated": "GENERATED BY DEFAULT AS IDENTITY"
            }
        ]
        
        indexes = []
        constraints = []
        
        # Add columns based on schema
        if analysis.schema and "columns" in analysis.schema:
            for col_info in analysis.schema["columns"]:
                column_def = {
                    "name": self._sanitize_column_name(col_info["name"]),
                    "type": col_info["type"],
                    "description": f"Original column: {col_info['name']}"
                }
                
                # Add constraints
                if not col_info.get("nullable", True):
                    column_def["constraints"] = ["NOT NULL"]
                
                columns.append(column_def)
                
                # Create indexes for potential key columns
                if col_info["name"].lower() in ["id", "name", "code", "key"]:
                    indexes.append({
                        "name": f"idx_{table_name}_{self._sanitize_column_name(col_info['name'])}",
                        "type": "BTREE",
                        "columns": [self._sanitize_column_name(col_info["name"])]
                    })
        
        # Add audit columns
        columns.extend([
            {
                "name": "created_at",
                "type": "TIMESTAMP",
                "constraints": ["NOT NULL"],
                "default": "CURRENT_TIMESTAMP"
            },
            {
                "name": "updated_at",
                "type": "TIMESTAMP",
                "default": "CURRENT_TIMESTAMP"
            }
        ])
        
        oracle_23ai_features = [
            "Wide Tables (up to 4096 columns)",
            "Boolean data type support",
            "Enhanced indexing capabilities"
        ]
        
        return TableSchema(
            table_name=table_name,
            columns=columns,
            indexes=indexes,
            constraints=constraints,
            oracle_23ai_features=oracle_23ai_features
        )
    
    def _generate_text_schema(self, analysis: LLMDataAnalysis, table_name: str) -> TableSchema:
        """Generate schema for text data with vector search capabilities"""
        columns = [
            {
                "name": "id",
                "type": "NUMBER",
                "constraints": ["PRIMARY KEY"],
                "generated": "GENERATED BY DEFAULT AS IDENTITY"
            },
            {
                "name": "content",
                "type": "CLOB",
                "constraints": ["NOT NULL"],
                "description": "Text content"
            },
            {
                "name": "content_vector",
                "type": "VECTOR",
                "description": "AI Vector representation for similarity search"
            },
            {
                "name": "content_hash",
                "type": "VARCHAR2(64)",
                "description": "Hash for duplicate detection"
            },
            {
                "name": "word_count",
                "type": "NUMBER",
                "description": "Number of words in content"
            },
            {
                "name": "created_at",
                "type": "TIMESTAMP",
                "constraints": ["NOT NULL"],
                "default": "CURRENT_TIMESTAMP"
            }
        ]
        
        # Add metadata columns if available
        if analysis.metadata:
            if "length" in analysis.metadata:
                columns.append({
                    "name": "content_length",
                    "type": "NUMBER",
                    "description": "Character count"
                })
        
        indexes = [
            {
                "name": f"idx_{table_name}_vector",
                "type": "VECTOR",
                "columns": ["content_vector"],
                "description": "Vector index for AI Vector Search"
            },
            {
                "name": f"idx_{table_name}_hash",
                "type": "BTREE",
                "columns": ["content_hash"],
                "description": "Index for duplicate detection"
            },
            {
                "name": f"idx_{table_name}_created",
                "type": "BTREE",
                "columns": ["created_at"]
            }
        ]
        
        constraints = [
            {
                "name": f"chk_{table_name}_word_count",
                "type": "CHECK",
                "condition": "word_count >= 0",
                "description": "Ensure word count is non-negative"
            }
        ]
        
        oracle_23ai_features = [
            "AI Vector Search",
            "Vector column type",
            "Vector indexes",
            "Enhanced CLOB handling"
        ]
        
        return TableSchema(
            table_name=table_name,
            columns=columns,
            indexes=indexes,
            constraints=constraints,
            oracle_23ai_features=oracle_23ai_features
        )
    
    def _generate_binary_schema(self, analysis: LLMDataAnalysis, table_name: str) -> TableSchema:
        """Generate schema for binary data using Value LOBs"""
        columns = [
            {
                "name": "id",
                "type": "NUMBER",
                "constraints": ["PRIMARY KEY"],
                "generated": "GENERATED BY DEFAULT AS IDENTITY"
            },
            {
                "name": "binary_data",
                "type": "BLOB",
                "constraints": ["NOT NULL"],
                "description": "Binary content"
            },
            {
                "name": "mime_type",
                "type": "VARCHAR2(100)",
                "description": "MIME type of the file"
            },
            {
                "name": "file_size",
                "type": "NUMBER",
                "description": "File size in bytes"
            },
            {
                "name": "file_hash",
                "type": "VARCHAR2(64)",
                "description": "SHA-256 hash for duplicate detection"
            },
            {
                "name": "created_at",
                "type": "TIMESTAMP",
                "constraints": ["NOT NULL"],
                "default": "CURRENT_TIMESTAMP"
            }
        ]
        
        # Add metadata columns if available
        if analysis.metadata:
            if "file_type" in analysis.metadata:
                columns.append({
                    "name": "file_type_description",
                    "type": "VARCHAR2(255)",
                    "description": "File type description"
                })
        
        indexes = [
            {
                "name": f"idx_{table_name}_mime_type",
                "type": "BTREE",
                "columns": ["mime_type"]
            },
            {
                "name": f"idx_{table_name}_file_hash",
                "type": "BTREE",
                "columns": ["file_hash"],
                "description": "Index for duplicate detection"
            },
            {
                "name": f"idx_{table_name}_created",
                "type": "BTREE",
                "columns": ["created_at"]
            }
        ]
        
        constraints = [
            {
                "name": f"chk_{table_name}_file_size",
                "type": "CHECK",
                "condition": "file_size >= 0",
                "description": "Ensure file size is non-negative"
            }
        ]
        
        oracle_23ai_features = [
            "Value LOBs for read-and-forget use cases",
            "Enhanced BLOB handling",
            "Improved LOB performance"
        ]
        
        return TableSchema(
            table_name=table_name,
            columns=columns,
            indexes=indexes,
            constraints=constraints,
            oracle_23ai_features=oracle_23ai_features
        )
    
    def _generate_generic_schema(self, analysis: LLMDataAnalysis, table_name: str) -> TableSchema:
        """Generate generic schema for unknown data types"""
        columns = [
            {
                "name": "id",
                "type": "NUMBER",
                "constraints": ["PRIMARY KEY"],
                "generated": "GENERATED BY DEFAULT AS IDENTITY"
            },
            {
                "name": "raw_data",
                "type": "CLOB",
                "constraints": ["NOT NULL"],
                "description": "Raw data content"
            },
            {
                "name": "data_type",
                "type": "VARCHAR2(50)",
                "description": "Detected data type"
            },
            {
                "name": "created_at",
                "type": "TIMESTAMP",
                "constraints": ["NOT NULL"],
                "default": "CURRENT_TIMESTAMP"
            }
        ]
        
        indexes = [
            {
                "name": f"idx_{table_name}_data_type",
                "type": "BTREE",
                "columns": ["data_type"]
            },
            {
                "name": f"idx_{table_name}_created",
                "type": "BTREE",
                "columns": ["created_at"]
            }
        ]
        
        
        oracle_23ai_features = [
            "Generic data storage",
            "Enhanced CLOB handling"
        ]
        
        return TableSchema(
            table_name=table_name,
            columns=columns,
            indexes=indexes,
            constraints=[],
            oracle_23ai_features=oracle_23ai_features
        )
    
    def _sanitize_column_name(self, name: str) -> str:
        """Sanitize column name for Oracle database"""
        # Remove special characters and replace with underscores
        import re
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        
        # Ensure it starts with a letter
        if sanitized and not sanitized[0].isalpha():
            sanitized = f"col_{sanitized}"
        
        # Ensure it's not empty
        if not sanitized:
            sanitized = "unnamed_column"
        
        # Truncate if too long
        if len(sanitized) > 30:
            sanitized = sanitized[:30]
        
        return sanitized.upper()
    
    def generate_ddl(self, schema: TableSchema) -> str:
        """Generate DDL statements for the schema"""
        ddl_parts = []
        
        # Create table statement
        ddl_parts.append(f"CREATE TABLE {schema.table_name} (")
        
        # Add columns
        column_definitions = []
        for column in schema.columns:
            col_def = f"    {column['name']} {column['type']}"
            
            if "generated" in column:
                col_def += f" {column['generated']}"
            
            if "default" in column:
                col_def += f" DEFAULT {column['default']}"
            
            if "constraints" in column:
                for constraint in column["constraints"]:
                    col_def += f" {constraint}"
            
            column_definitions.append(col_def)
        
        ddl_parts.append(",\n".join(column_definitions))
        ddl_parts.append(")")
        
        create_table_stmt = "".join(ddl_parts)
        ddl_parts = []
        ddl_parts.append(create_table_stmt)
        # Add constraints
        for constraint in schema.constraints:
            constraint_ddl = f"ALTER TABLE {schema.table_name} ADD CONSTRAINT {constraint['name']} "
            constraint_ddl += f"{constraint['type']} ({constraint['condition']})"
            ddl_parts.append(constraint_ddl)
        
        # Add indexes
        for index in schema.indexes:
            if index["type"] == "VECTOR":
                index_ddl = f"CREATE VECTOR INDEX {index['name']} ON {schema.table_name} "
                index_ddl += f"({', '.join(index['columns'])}) ORGANIZATION INMEMORY NEIGHBOR GRAPH DISTANCE COSINE WITH TARGET ACCURACY 90"
                ddl_parts.append(index_ddl)
            else:
                index_ddl = f"CREATE INDEX {index['name']} ON {schema.table_name} "
                index_ddl += f"({', '.join(index['columns'])})"
                ddl_parts.append(index_ddl)
        return ";\n\n".join(ddl_parts)

